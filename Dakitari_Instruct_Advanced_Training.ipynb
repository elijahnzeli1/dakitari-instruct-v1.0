{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Dakitari-Instruct Advanced Training in Google Colab\n",
        "\n",
        "This notebook implements advanced training techniques including:\n",
        "- Supervised fine-tuning\n",
        "- Self-supervised learning\n",
        "- Reinforcement learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set up project directories\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Dakitari-Instruct'\n",
        "DATASET_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
        "MODEL_CHECKPOINT_DIR = os.path.join(PROJECT_DIR, 'checkpoints')\n",
        "\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository, overwriting if it exists\n",
        "%cd /content\n",
        "!rm -rf dakitari-instruct-v1.5 # Remove existing directory\n",
        "!git clone https://github.com/elijahnzeli1/dakitari-instruct-v1.5.git\n",
        "%cd dakitari-instruct-v1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced-training"
      },
      "outputs": [],
      "source": [
        "# Run advanced training with increased model capacity\n",
        "!python advanced_training.py \\\n",
        "    --model_dir checkpoints/Dakitari-instruct-v1.0 \\\n",
        "    --output_dir {MODEL_CHECKPOINT_DIR}/Dakitari-instruct-v1.5 \\\n",
        "    --batch_size 16 \\\n",
        "    --epochs 5 \\\n",
        "    --max_length 1024 \\\n",
        "    --embed_dim 1024 \\\n",
        "    --num_heads 16 \\\n",
        "    --ff_dim 4096 \\\n",
        "    --num_transformer_blocks 24 \\\n",
        "    --dropout_rate 0.1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_experts 8 \\\n",
        "    --expert_capacity 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the finetune.py script with the updated parameters\n",
        "!python finetune.py \\\n",
        "    --model_dir checkpoints/Dakitari-instruct-v1.2 \\\n",
        "    --output_dir {MODEL_CHECKPOINT_DIR}/Dakitari-instruct-v1.5 \\ \n",
        "    --epochs 3 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --max_length 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "# Create zip of trained model\n",
        "!zip -r dakitari_instruct_v1.5.zip {MODEL_CHECKPOINT_DIR}/Dakitari-instruct-v1.5\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "files.download('dakitari_instruct_v1.5.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "!python evaluate.py \\\n",
        "    --model_path {os.path.join(MODEL_CHECKPOINT_DIR, 'Dakitari-instruct-v1.5/model.safetensors')} \\\n",
        "    --max_length 1024 \\\n",
        "    --batch_size 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat with the trained model\n",
        "!python chat.py \\\n",
        "    --model_dir {MODEL_CHECKPOINT_DIR}/Dakitari-instruct-v1.5 \\\n",
        "    --temperature 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For a more interactive chat experience in the notebook\n",
        "import sys\n",
        "sys.path.append('/content/Dakitari-Instruct')\n",
        "\n",
        "from model.transformer_model import DakitariInstructModel\n",
        "from transformers import AutoTokenizer\n",
        "import tensorflow as tf\n",
        "from chat import load_model, generate_response\n",
        "\n",
        "def chat_interface():\n",
        "    model_dir = f\"{MODEL_CHECKPOINT_DIR}/Dakitari-instruct-v1.5\"\n",
        "    temperature = 0.8\n",
        "    \n",
        "    print(\"Loading model...\")\n",
        "    model, tokenizer = load_model(model_dir)\n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    print(\"\\nWelcome to Dakitari-Instruct Chat!\")\n",
        "    print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
        "    print(\"Type 'clear' to clear the conversation history.\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "        \n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "            \n",
        "        if user_input.lower() == 'clear':\n",
        "            print(\"\\nConversation cleared.\")\n",
        "            continue\n",
        "            \n",
        "        if not user_input:\n",
        "            continue\n",
        "            \n",
        "        print(\"\\nThinking...\")\n",
        "        response = generate_response(model, tokenizer, user_input, temperature=temperature)\n",
        "        print(f\"\\nDakitari: {response}\")\n",
        "\n",
        "# Run the chat interface\n",
        "chat_interface()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
